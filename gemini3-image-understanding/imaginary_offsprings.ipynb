{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b86abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install google-genai\n",
    "%pip install matplotlib\n",
    "%pip install os\n",
    "%pip install dotenv\n",
    "%pip install pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a1d6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from dotenv import load_dotenv\n",
    "from google.genai import types\n",
    "from pydantic import BaseModel, Field\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a20ef97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GendersVerification(BaseModel):\n",
    "    result: bool = Field(description=\"Whether the genders are opposite.\")\n",
    "    stop_reason: str | None = Field(None, description=\"The stop reason when result is False.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fb24b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_genai_client():\n",
    "    import os\n",
    "\n",
    "    api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise ValueError(\"GOOGLE_API_KEY not found in .env file\")    \n",
    "\n",
    "    # Configure the client with your API key\n",
    "    client = genai.Client(api_key=api_key, http_options={'api_version': 'v1alpha'})\n",
    "\n",
    "    return client\n",
    "\n",
    "def create_vertexai_client():\n",
    "    import os\n",
    "    \n",
    "    cloud_api_key = os.getenv(\"GOOGLE_CLOUD_API_KEY\")\n",
    "    if not cloud_api_key:\n",
    "        raise ValueError(\"GOOGLE_CLOUD_API_KEY not found in .env file\")\n",
    "    \n",
    "    # Configure the client with your API key\n",
    "    client = genai.Client(\n",
    "        vertexai=True, \n",
    "        api_key=cloud_api_key, \n",
    "    )\n",
    "\n",
    "    return client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4556c64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path: str):\n",
    "    try:\n",
    "        img = mpimg.imread(image_path)\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file at '{image_path}' was not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578874af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inline_data_part(image_path: str):\n",
    "    import mimetypes\n",
    "\n",
    "    try:\n",
    "        mime_type, _ = mimetypes.guess_type(image_path)\n",
    "        if mime_type is None:\n",
    "            mime_type = 'application/octet-stream'\n",
    "            print(f\"Warning: Could not determine MIME type for {image_path}. Defaulting to {mime_type}.\")\n",
    "\n",
    "        file_bytes: bytes | None = None\n",
    "        with open(image_path, \"rb\") as image_file:\n",
    "            file_bytes = image_file.read()\n",
    "        \n",
    "        if file_bytes is None:\n",
    "            raise Exception(f\"Unable to read the bytes from {image_path}\")\n",
    "    \n",
    "        return types.Part(\n",
    "            inline_data=types.Blob(\n",
    "                mime_type=mime_type,\n",
    "                data=file_bytes\n",
    "            ),\n",
    "            media_resolution={\"level\": \"media_resolution_high\"}\n",
    "        ) \n",
    "    except FileNotFoundError:\n",
    "        print (f\"Error: The file was not found at {image_path}\")\n",
    "    except Exception as e:\n",
    "        print (f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57b55c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_token_usage(response: types.GenerateContentResponse):\n",
    "    if response and response.usage_metadata:\n",
    "        usage_metadata = response.usage_metadata\n",
    "        input_token_count = usage_metadata.prompt_token_count\n",
    "        output_token_count = usage_metadata.candidates_token_count\n",
    "        total_token_count = usage_metadata.total_token_count\n",
    "        thought_token_count = usage_metadata.thoughts_token_count\n",
    "        cached_token_count = usage_metadata.cached_content_token_count\n",
    "        print(f\"Input: {input_token_count}, Output: {output_token_count}, Thought: {thought_token_count}, Cached: {cached_token_count} Total: {total_token_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f44d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "# Configure the client with your API key\n",
    "# client = create_genai_client()\n",
    "\n",
    "client = create_vertexai_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae79fd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [types.Tool(google_search=types.GoogleSearch())]\n",
    "\n",
    "def verify_genders(person_a_image: str, person_b_image: str):\n",
    "    def clean_json_string(raw_string):\n",
    "        # Remove the markdown code blocks\n",
    "        clean_str = raw_string.strip()\n",
    "        if clean_str.startswith(\"```json\"):\n",
    "            clean_str = clean_str[7:]\n",
    "        if clean_str.endswith(\"```\"):\n",
    "            clean_str = clean_str[:-3]\n",
    "        return clean_str.strip()\n",
    "    \n",
    "    gender_verification_prompt = \"\"\"\n",
    "    Role: You are an Advanced Image Content Validator. Your goal is to identify human subjects for a biological analysis tool, even in complex images.\n",
    "\n",
    "    Task: Analyze two input images (Image A and Image B). Locate the primary human subject in each image and verify their genders.\n",
    "\n",
    "    Validation & Selection Logic:\n",
    "\n",
    "    1. Smart Subject Detection:\n",
    "    - Scan each image for a human face or figure.\n",
    "    - Crucial: If an image contains both humans and objects (e.g., a person holding a guitar, a person next to a car, or a person in a cluttered room), you must ignore the objects and focus exclusively on the human.\n",
    "    - If multiple humans are present, select the most prominent/clearest face as the subject for that image.\n",
    "    - Failure Condition: If no recognizable human face is found in one or both images (e.g., only a landscape, animal, or object is visible), set \"result\" to false and \"stop_reason\" to \"One or both images do not contain a detectable human face.\"\n",
    "\n",
    "    2. Gender Verification:\n",
    "    - Analyze the biological sex or gender presentation of the selected human subject in Image A and Image B.\n",
    "    - Failure Condition: If the subjects are of the same gender (Male+Male or Female+Female), set \"result\" to false and \"stop_reason\" to \"Please upload one male and one female.\"\n",
    "\n",
    "    3. Pass Condition:\n",
    "    - If both images contain a human subject AND they are of opposite genders (One Male + One Female), set \"result\" to true and \"stop_reason\" to null.\n",
    "\n",
    "    Output Schema:\n",
    "    Return ONLY a single JSON object with no markdown formatting or additional text.\n",
    "\n",
    "    {\n",
    "    \"result\": boolean,\n",
    "    \"stop_reason\": string | null\n",
    "    }\n",
    "    \"\"\"\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-3-flash-preview\",\n",
    "        contents=[\n",
    "            types.Content(\n",
    "                role=\"user\",\n",
    "                parts=[\n",
    "                    types.Part(text=gender_verification_prompt),\n",
    "                    get_inline_data_part(person_a_image),\n",
    "                    get_inline_data_part(person_b_image),\n",
    "                ]\n",
    "            )\n",
    "        ],\n",
    "        config=types.GenerateContentConfig(\n",
    "            response_mime_type=\"application/json\",\n",
    "            response_json_schema=GendersVerification.model_json_schema(),\n",
    "            tools=tools,\n",
    "            media_resolution=types.MediaResolution.MEDIA_RESOLUTION_HIGH,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    print_token_usage(response)\n",
    "    result = GendersVerification.model_validate_json(clean_json_string(response.text))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9341c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sibling_images(person_a_image: str, person_b_image: str):\n",
    "    prompt = \"\"\"\n",
    "    A group portrait of four distinct young adult siblings (two males, two females, aged 18-22). \n",
    "    They are the biological offspring of the provided references, generated with **high genetic variability**. \n",
    "    The faces must display a **randomized distribution of traits**, where features (eyes, nose, mouth, jawline) are mixed **unequally** across the four subjects. \n",
    "    Ensure distinctiveness: Sibling 1 should lean strongly towards the father's features, Sibling 2 towards the mother's, while Siblings 3 and 4 represent complex, unique mixes of recessive and dominant traits. \n",
    "    **Crucial:** All four must have fresh, smooth, youthful skin and collegiate appearances. \n",
    "    Do not transfer the parents' wrinkles or skin texture. 2k resolution, photorealistic, detailed distinct faces.\n",
    "    \"\"\"\n",
    "\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-3-pro-image-preview\",\n",
    "        contents=[\n",
    "            types.Content(\n",
    "                parts=[\n",
    "                    types.Part(text=prompt),\n",
    "                    get_inline_data_part(person_a_image),\n",
    "                    get_inline_data_part(person_b_image),\n",
    "                ]\n",
    "            )\n",
    "        ],\n",
    "        config=types.GenerateContentConfig(\n",
    "            response_modalities=['TEXT', 'IMAGE'],\n",
    "            image_config=types.ImageConfig(\n",
    "                image_size=\"2K\",  \n",
    "            ),\n",
    "            tools=tools,\n",
    "            thinking_config=types.ThinkingConfig(\n",
    "                include_thoughts=True\n",
    "            ),\n",
    "            temperature=0.5\n",
    "        )\n",
    "    )\n",
    "\n",
    "    print_token_usage(response)\n",
    "\n",
    "    image_bytes: bytes | None = None\n",
    "    if response.candidates and response.candidates[0].content and response.candidates[0].content.parts:\n",
    "        for part in response.candidates[0].content.parts:\n",
    "            if part.thought and part.text:\n",
    "                display(Markdown(f\"Though Summary:\\n {part.text}\"))\n",
    "            elif part.text:\n",
    "                print(\"Text: \", part.text)\n",
    "            if part.inline_data:\n",
    "                image_bytes = part.inline_data.data\n",
    "       \n",
    "    return image_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b56c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_result(person_a_image: str, person_b_image: str):\n",
    "    load_image(person_a_image)\n",
    "    load_image(person_b_image)\n",
    "\n",
    "    result = verify_genders(person_a_image=person_a_image, person_b_image=person_b_image)\n",
    "    if result.result:\n",
    "        offspring_bytes = generate_sibling_images(person_a_image, person_b_image)\n",
    "        offspring_image = Image.open(BytesIO(offspring_bytes)) if offspring_bytes else None\n",
    "        if offspring_image:\n",
    "            plt.imshow(offspring_image)\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "        pass\n",
    "    else:\n",
    "        if result.stop_reason:\n",
    "            print (\"Stop reason:\", result.stop_reason)\n",
    "\n",
    "def print_test_cases(heading: str, cases: list[list[str]]):\n",
    "    print(heading)\n",
    "    for case in cases:\n",
    "        print_result(person_a_image=case[0], person_b_image=case[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a02da48",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_test_cases(heading=\"Same gender cases (Expected failure)\", cases=[\n",
    "    ['./couples/prince_william.jpg', './couples/david_beckham.webp']\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdac7e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_test_cases(heading=\"Couple cases\", cases=[\n",
    "    ['./couples/prince_william.jpg', './couples/princess_kate.jpg'],\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d8c8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_test_cases(heading=\"Couple cases\", cases=[\n",
    "    ['./couples/victoria_beckham.jpg', './couples/david_beckham.webp']\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dd4ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_test_cases(heading=\"Couple cases\", cases=[\n",
    "    ['./couples/Shizuka_Kudo.jpg', './couples/Takuya_Kimura.jpg'],\n",
    "])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
