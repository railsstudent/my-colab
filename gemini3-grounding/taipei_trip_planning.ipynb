{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b86abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install google-genai\n",
    "%pip install matplotlib\n",
    "%pip install os\n",
    "%pip install dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "46a1d6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from dotenv import load_dotenv\n",
    "from google.genai import types\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fb24b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_genai_client():\n",
    "    import os\n",
    "\n",
    "    # api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "    # if not api_key:\n",
    "    #     raise ValueError(\"GOOGLE_API_KEY not found in .env file\")\n",
    "    \n",
    "    cloud_api_key = os.getenv(\"GOOGLE_CLOUD_API_KEY\")\n",
    "    if not cloud_api_key:\n",
    "        raise ValueError(\"GOOGLE_CLOUD_API_KEY not found in .env file\")\n",
    "    \n",
    "    # Configure the client with your API key\n",
    "    # client = genai.Client(api_key=api_key, http_options={'api_version': 'v1alpha'})\n",
    "    client = genai.Client(\n",
    "        vertexai=True, \n",
    "        api_key=cloud_api_key, \n",
    "    )\n",
    "\n",
    "    return client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73765af",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "# Configure the client with your API key\n",
    "client = create_genai_client()\n",
    "\n",
    "tools = [types.Tool(google_search=types.GoogleSearch())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bc863e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_token_usage(response: types.GenerateContentResponse):\n",
    "    if response and response.usage_metadata:\n",
    "        usage_metadata = response.usage_metadata\n",
    "        input_token_count = usage_metadata.prompt_token_count\n",
    "        output_token_count = usage_metadata.candidates_token_count\n",
    "        total_token_count = usage_metadata.total_token_count\n",
    "        thought_token_count = usage_metadata.thoughts_token_count\n",
    "        cached_token_count = usage_metadata.cached_content_token_count\n",
    "        print(f\"Input: {input_token_count}, Output: {output_token_count}, Thought: {thought_token_count}, Cached: {cached_token_count} Total: {total_token_count}\")\n",
    "        \n",
    "def get_num_citations(grounding_supports: list[types.GroundingSupport] | None):\n",
    "    num_citations = 0\n",
    "    for support in grounding_supports or []:\n",
    "        if support.grounding_chunk_indices:\n",
    "            num_citations = num_citations + len(support.grounding_chunk_indices)\n",
    "\n",
    "    return num_citations\n",
    "\n",
    "def build_citations(grounding_supports: list[types.GroundingSupport] | None, grounding_chunks: list[types.GroundingChunk] | None):\n",
    "    citations: list[str] = []\n",
    "    for support in grounding_supports:\n",
    "        if support.grounding_chunk_indices:\n",
    "            for i in support.grounding_chunk_indices:\n",
    "                uri = ''\n",
    "                if grounding_chunks and i < len(grounding_chunks):\n",
    "                    chunk = grounding_chunks[i]\n",
    "                    uri = chunk.web.uri if chunk.web and chunk.web.uri else ''\n",
    "                if uri:\n",
    "                    citations.append(uri)\n",
    "\n",
    "    text_citations = \"\\nCitation link: \".join(citations)\n",
    "    return \"Citation link: \" + text_citations\n",
    "\n",
    "def execute_prompt(prompt: str):\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-3-pro-preview\",\n",
    "        contents=[types.Content(\n",
    "            role=\"user\",\n",
    "            parts=[types.Part(text=prompt)]\n",
    "        )],\n",
    "        config=types.GenerateContentConfig(\n",
    "            tools=tools,\n",
    "            thinking_config=types.ThinkingConfig(\n",
    "                thinking_level=types.ThinkingLevel.HIGH\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    print_token_usage(response)\n",
    "    return response\n",
    "\n",
    "def print_response(response: types.GenerateContentResponse):\n",
    "    # print the response\n",
    "    display(Markdown(f\"Response:\\n {response.text}\"))\n",
    "\n",
    "    grounding_metadata = response.candidates[0].grounding_metadata if response.candidates and response.candidates[0] and response.candidates[0].grounding_metadata else None\n",
    "    grounding_chunks = grounding_metadata.grounding_chunks if grounding_metadata and grounding_metadata.grounding_chunks else None\n",
    "    grounding_supports = grounding_metadata.grounding_supports if grounding_metadata and grounding_metadata.grounding_supports else None\n",
    "    if grounding_supports and grounding_chunks:\n",
    "        print (f\"{get_num_citations(grounding_supports)} Citations\")\n",
    "        citations = build_citations(grounding_supports=grounding_supports, grounding_chunks=grounding_chunks)\n",
    "        print(citations)\n",
    "\n",
    "    web_search_queries = grounding_metadata.web_search_queries if grounding_metadata and grounding_metadata.web_search_queries else None\n",
    "    if web_search_queries and len(web_search_queries) > 0:\n",
    "        for query in web_search_queries:\n",
    "            if query:\n",
    "                print (f\"Query -> {query}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b56c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = execute_prompt(prompt=\"\"\"\n",
    "    Please search for the latest confirmed weather forecast for Taiwan for the dates listed below in 2025. specifically looking at Taipei and Taoyuan.\n",
    "    \n",
    "    Present the results in a single clear table with the following columns: Date, Location, Weather (Condition, Temp in Â°C, Precipitation %), and Appropriate Clothing.\n",
    "\n",
    "    The dates and locations are:\n",
    "    1. Taipei: November 26, 27, 28, and 29.\n",
    "    2. Taoyuan: November 30 and December 1.\n",
    "\"\"\")\n",
    "\n",
    "print_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1273799a",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_forecast = response.text\n",
    "response = execute_prompt(prompt=f\"\"\"\n",
    "    Based on the weather forecast you just retrieved and the itinerary details below, please act as a professional travel planner and generate a comprehensive packing list.\n",
    "\n",
    "    Trip Itinerary:\n",
    "    - November 26, 2025 - November 29, 2025: Taipei (City exploration, night markets, extensive walking).\n",
    "    - November 30, 2025 - December 1, 2025: Taoyuan (Transitioning closer to the airport/coastal area).\n",
    "\n",
    "    Please categorize the packing list into:\n",
    "\n",
    "    1. Clothing: Specific recommendations based on the temperature and rain forecast you found. Suggest layers suitable for transitioning between humid outdoor weather, likely rain, and strong indoor air conditioning. Recommend shoes suitable for wet pavement and high daily step counts.\n",
    "    2. Documents: Entry requirements for Taiwan, digital backups, and essential travel apps.\n",
    "    3. Financial: Advice on carrying cash (TWD) vs. credit cards, specifically distinguishing between Night Market needs vs. Malls/Department stores.\n",
    "    4. Electronics: Adapter types (confirm if US plugs work in Taiwan), power banks, and connectivity (eSIM vs. physical SIM).\n",
    "    5. Essentials: Toiletries, Feminine products and specific items for humid weather. \n",
    "    6. Taoyuan Logistics: Any specific tips for moving between Taipei and Taoyuan (e.g., keeping travel documents accessible).\n",
    "    7. Airport Logistics: Any specific tips for moving between the hotel I am staying in Taoyuan and the airport.\n",
    "\n",
    "    Special Request: Include a \"Taiwan Essentials\" section, including the EasyCard (transport card) and specific umbrella recommendations.\n",
    "    \n",
    "    Weather forecast:\n",
    "    {weather_forecast}\n",
    "    \"\"\")\n",
    "\n",
    "print_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cfc165",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = execute_prompt(prompt=\"\"\"\n",
    "    Please search the web for my Cathay Pacific flight on November 26, 2025 from Hong Kong to Taipei. The flight number is CX530.\n",
    "    The departure flight is CX495 on December 1, 2025. \n",
    "      \n",
    "    Present the results in a single clear table with the following columns: Date, From, To, Flight Provider, Flight  Number, Plane Model, Departure, Arrival.\n",
    "    The From column includes location, airport code, and terminal, in the format of  <Location> (<Code>) - <Terminal>\n",
    "    Similarly, the To column includes the the same information in the same format.\"\"\"\n",
    ")\n",
    "\n",
    "print_response(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
